{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcvZD1kS6hsbaBy/LmJA17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-727/Reinforcement-Learning-/blob/main/2348544_Lab10_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WFixgSrxSpFZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Network\n",
        "* Policy network is used to map states to action probabilities"
      ],
      "metadata": {
        "id": "wR1WjW_FS8Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)"
      ],
      "metadata": {
        "id": "NzRgrVunS4r-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Gradient Agent\n",
        "* Selects actions using the policy network.\n",
        "* Records rewards and log probabilities of actions.\n",
        "* Updates the policy using the policy gradient theorem."
      ],
      "metadata": {
        "id": "D7a_VENRTEgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyGradientAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=0.01):\n",
        "        self.policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.episode_rewards = []\n",
        "        self.episode_log_probs = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs = self.policy_net(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        self.episode_log_probs.append(dist.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "    def record_reward(self, reward):\n",
        "        self.episode_rewards.append(reward)\n",
        "\n",
        "    def update_policy(self):\n",
        "        R = 0\n",
        "        discounted_rewards = []\n",
        "        for reward in reversed(self.episode_rewards):\n",
        "            R = reward + 0.99 * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
        "        policy_loss = []\n",
        "        for log_prob, reward in zip(self.episode_log_probs, discounted_rewards):\n",
        "            policy_loss.append((-log_prob * reward).unsqueeze(0))\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_log_probs = []"
      ],
      "metadata": {
        "id": "auSFA3xRS-TM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below is a reinforcement learning agent using the Policy Gradient method to solve the CartPole-v1 environment\n",
        "* CartPole environment, where the agent must balance a pole on a cart by moving left or right.\n",
        "* The policy is updated using rewards obtained during episodes to improve the agent's ability to balance the pole. Training stops when the agent consistently performs at or above the target reward for a specified number of episodes."
      ],
      "metadata": {
        "id": "ALnrGpBpZ_SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import gym\n",
        "\n",
        "    env = gym.make('CartPole-v1')\n",
        "    agent = PolicyGradientAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "\n",
        "    target_reward = 195\n",
        "    patience = 10  # Number of episodes to observe consistent performance\n",
        "\n",
        "    consecutive_wins = 0\n",
        "    for episode in range(1000):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(1, 10000):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.record_reward(reward)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.update_policy()\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if episode_reward >= target_reward:\n",
        "            consecutive_wins += 1\n",
        "            if consecutive_wins >= patience:\n",
        "                print(f\"Environment solved in {episode + 1} episodes!\")\n",
        "                break\n",
        "        else:\n",
        "            consecutive_wins = 0\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uou7MqAoTHfQ",
        "outputId": "4149f531-7bbf-42ab-fd24-70963e5000dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = 30.0\n",
            "Episode 2: Total Reward = 41.0\n",
            "Episode 3: Total Reward = 12.0\n",
            "Episode 4: Total Reward = 18.0\n",
            "Episode 5: Total Reward = 33.0\n",
            "Episode 6: Total Reward = 10.0\n",
            "Episode 7: Total Reward = 14.0\n",
            "Episode 8: Total Reward = 10.0\n",
            "Episode 9: Total Reward = 9.0\n",
            "Episode 10: Total Reward = 11.0\n",
            "Episode 11: Total Reward = 10.0\n",
            "Episode 12: Total Reward = 10.0\n",
            "Episode 13: Total Reward = 10.0\n",
            "Episode 14: Total Reward = 13.0\n",
            "Episode 15: Total Reward = 12.0\n",
            "Episode 16: Total Reward = 12.0\n",
            "Episode 17: Total Reward = 10.0\n",
            "Episode 18: Total Reward = 12.0\n",
            "Episode 19: Total Reward = 11.0\n",
            "Episode 20: Total Reward = 12.0\n",
            "Episode 21: Total Reward = 11.0\n",
            "Episode 22: Total Reward = 12.0\n",
            "Episode 23: Total Reward = 10.0\n",
            "Episode 24: Total Reward = 22.0\n",
            "Episode 25: Total Reward = 10.0\n",
            "Episode 26: Total Reward = 10.0\n",
            "Episode 27: Total Reward = 20.0\n",
            "Episode 28: Total Reward = 13.0\n",
            "Episode 29: Total Reward = 27.0\n",
            "Episode 30: Total Reward = 19.0\n",
            "Episode 31: Total Reward = 26.0\n",
            "Episode 32: Total Reward = 12.0\n",
            "Episode 33: Total Reward = 23.0\n",
            "Episode 34: Total Reward = 28.0\n",
            "Episode 35: Total Reward = 36.0\n",
            "Episode 36: Total Reward = 48.0\n",
            "Episode 37: Total Reward = 27.0\n",
            "Episode 38: Total Reward = 17.0\n",
            "Episode 39: Total Reward = 11.0\n",
            "Episode 40: Total Reward = 19.0\n",
            "Episode 41: Total Reward = 40.0\n",
            "Episode 42: Total Reward = 25.0\n",
            "Episode 43: Total Reward = 19.0\n",
            "Episode 44: Total Reward = 20.0\n",
            "Episode 45: Total Reward = 25.0\n",
            "Episode 46: Total Reward = 24.0\n",
            "Episode 47: Total Reward = 25.0\n",
            "Episode 48: Total Reward = 21.0\n",
            "Episode 49: Total Reward = 26.0\n",
            "Episode 50: Total Reward = 18.0\n",
            "Episode 51: Total Reward = 12.0\n",
            "Episode 52: Total Reward = 23.0\n",
            "Episode 53: Total Reward = 33.0\n",
            "Episode 54: Total Reward = 20.0\n",
            "Episode 55: Total Reward = 13.0\n",
            "Episode 56: Total Reward = 16.0\n",
            "Episode 57: Total Reward = 13.0\n",
            "Episode 58: Total Reward = 13.0\n",
            "Episode 59: Total Reward = 23.0\n",
            "Episode 60: Total Reward = 19.0\n",
            "Episode 61: Total Reward = 13.0\n",
            "Episode 62: Total Reward = 21.0\n",
            "Episode 63: Total Reward = 37.0\n",
            "Episode 64: Total Reward = 19.0\n",
            "Episode 65: Total Reward = 22.0\n",
            "Episode 66: Total Reward = 27.0\n",
            "Episode 67: Total Reward = 27.0\n",
            "Episode 68: Total Reward = 25.0\n",
            "Episode 69: Total Reward = 24.0\n",
            "Episode 70: Total Reward = 19.0\n",
            "Episode 71: Total Reward = 26.0\n",
            "Episode 72: Total Reward = 20.0\n",
            "Episode 73: Total Reward = 28.0\n",
            "Episode 74: Total Reward = 103.0\n",
            "Episode 75: Total Reward = 58.0\n",
            "Episode 76: Total Reward = 92.0\n",
            "Episode 77: Total Reward = 63.0\n",
            "Episode 78: Total Reward = 38.0\n",
            "Episode 79: Total Reward = 77.0\n",
            "Episode 80: Total Reward = 46.0\n",
            "Episode 81: Total Reward = 68.0\n",
            "Episode 82: Total Reward = 42.0\n",
            "Episode 83: Total Reward = 130.0\n",
            "Episode 84: Total Reward = 70.0\n",
            "Episode 85: Total Reward = 99.0\n",
            "Episode 86: Total Reward = 64.0\n",
            "Episode 87: Total Reward = 68.0\n",
            "Episode 88: Total Reward = 62.0\n",
            "Episode 89: Total Reward = 120.0\n",
            "Episode 90: Total Reward = 80.0\n",
            "Episode 91: Total Reward = 119.0\n",
            "Episode 92: Total Reward = 212.0\n",
            "Episode 93: Total Reward = 91.0\n",
            "Episode 94: Total Reward = 89.0\n",
            "Episode 95: Total Reward = 69.0\n",
            "Episode 96: Total Reward = 65.0\n",
            "Episode 97: Total Reward = 149.0\n",
            "Episode 98: Total Reward = 81.0\n",
            "Episode 99: Total Reward = 166.0\n",
            "Episode 100: Total Reward = 84.0\n",
            "Episode 101: Total Reward = 84.0\n",
            "Episode 102: Total Reward = 79.0\n",
            "Episode 103: Total Reward = 113.0\n",
            "Episode 104: Total Reward = 119.0\n",
            "Episode 105: Total Reward = 127.0\n",
            "Episode 106: Total Reward = 114.0\n",
            "Episode 107: Total Reward = 49.0\n",
            "Episode 108: Total Reward = 131.0\n",
            "Episode 109: Total Reward = 82.0\n",
            "Episode 110: Total Reward = 91.0\n",
            "Episode 111: Total Reward = 76.0\n",
            "Episode 112: Total Reward = 137.0\n",
            "Episode 113: Total Reward = 126.0\n",
            "Episode 114: Total Reward = 138.0\n",
            "Episode 115: Total Reward = 164.0\n",
            "Episode 116: Total Reward = 176.0\n",
            "Episode 117: Total Reward = 154.0\n",
            "Episode 118: Total Reward = 148.0\n",
            "Episode 119: Total Reward = 131.0\n",
            "Episode 120: Total Reward = 135.0\n",
            "Episode 121: Total Reward = 101.0\n",
            "Episode 122: Total Reward = 116.0\n",
            "Episode 123: Total Reward = 119.0\n",
            "Episode 124: Total Reward = 101.0\n",
            "Episode 125: Total Reward = 96.0\n",
            "Episode 126: Total Reward = 83.0\n",
            "Episode 127: Total Reward = 108.0\n",
            "Episode 128: Total Reward = 112.0\n",
            "Episode 129: Total Reward = 100.0\n",
            "Episode 130: Total Reward = 115.0\n",
            "Episode 131: Total Reward = 185.0\n",
            "Episode 132: Total Reward = 135.0\n",
            "Episode 133: Total Reward = 137.0\n",
            "Episode 134: Total Reward = 149.0\n",
            "Episode 135: Total Reward = 221.0\n",
            "Episode 136: Total Reward = 153.0\n",
            "Episode 137: Total Reward = 303.0\n",
            "Episode 138: Total Reward = 316.0\n",
            "Episode 139: Total Reward = 466.0\n",
            "Episode 140: Total Reward = 500.0\n",
            "Episode 141: Total Reward = 500.0\n",
            "Episode 142: Total Reward = 500.0\n",
            "Episode 143: Total Reward = 500.0\n",
            "Episode 144: Total Reward = 500.0\n",
            "Episode 145: Total Reward = 500.0\n",
            "Episode 146: Total Reward = 448.0\n",
            "Environment solved in 146 episodes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The training process demonstrates that the policy gradient algorithm successfully learned to solve the environment in 146 episodes\n",
        "* The progression is shown around Episode 74, a significant improvement in performance begins, with rewards exceeding 100. This shows the agent is starting to identify strategies that result in better outcomes."
      ],
      "metadata": {
        "id": "roK1zu42WFBG"
      }
    }
  ]
}