{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C2QvQ5wovDUi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class MonteCarlo:\n",
        "    def __init__(self, env, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the Monte Carlo instance.\n",
        "        :param env: OpenAI Gym-like environment.\n",
        "        :param gamma: Discount factor.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.returns = defaultdict(list)  # Store returns for each state-action pair\n",
        "        self.state_values = defaultdict(float)  # For prediction\n",
        "        self.action_values = defaultdict(float)  # For control\n",
        "        self.policy = defaultdict(lambda: random.choice(range(env.action_space.n)))  # Random initial policy\n",
        "\n",
        "    def generate_episode(self, policy):\n",
        "        \"\"\"\n",
        "        Generate an episode following the policy.\n",
        "        :param policy: Dictionary mapping state to action.\n",
        "        :return: List of (state, action, reward) tuples.\n",
        "        \"\"\"\n",
        "        episode = []\n",
        "        state = self.env.reset()\n",
        "        while True:\n",
        "            action = policy[state]\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        return episode\n",
        "\n",
        "    def mc_prediction(self, episodes=500):\n",
        "        \"\"\"\n",
        "        Monte Carlo Prediction for estimating state-value function V(s).\n",
        "        :param episodes: Number of episodes to sample.\n",
        "        \"\"\"\n",
        "        for _ in range(episodes):\n",
        "            episode = self.generate_episode(self.policy)\n",
        "            G = 0\n",
        "            visited_states = set()\n",
        "            for t in reversed(range(len(episode))):\n",
        "                state, _, reward = episode[t]\n",
        "                G = self.gamma * G + reward\n",
        "                if state not in visited_states:  # First-visit MC\n",
        "                    visited_states.add(state)\n",
        "                    self.returns[state].append(G)\n",
        "                    self.state_values[state] = np.mean(self.returns[state])\n",
        "\n",
        "    def mc_control(self, episodes=500):\n",
        "        \"\"\"\n",
        "        Monte Carlo Control using Exploring Starts to find the optimal policy.\n",
        "        :param episodes: Number of episodes to sample.\n",
        "        \"\"\"\n",
        "        for _ in range(episodes):\n",
        "            episode = self.generate_episode(self.policy)\n",
        "            G = 0\n",
        "            visited_state_actions = set()\n",
        "            for t in reversed(range(len(episode))):\n",
        "                state, action, reward = episode[t]\n",
        "                G = self.gamma * G + reward\n",
        "                if (state, action) not in visited_state_actions:  # First-visit MC\n",
        "                    visited_state_actions.add((state, action))\n",
        "                    self.returns[(state, action)].append(G)\n",
        "                    self.action_values[(state, action)] = np.mean(self.returns[(state, action)])\n",
        "\n",
        "                    # Update policy to be greedy w.r.t the action-value function\n",
        "                    state_actions = [self.action_values[(state, a)] for a in range(self.env.action_space.n)]\n",
        "                    self.policy[state] = np.argmax(state_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "\n",
        "Monte-carlo prediction\n",
        "\n",
        "Goal: Learn how good it is to be in a certain state when following a specific policy.\n",
        "\n",
        "Imagine playing a game (like Blackjack) over and over while following a certain strategy (policy).\n",
        "Every time you reach a state (e.g., your current cards in Blackjack), you note down the eventual total reward you get by the end of the game.\n",
        "After enough games, you take the average of all these total rewards for each state. *This average becomes your estimate of how valuable that state is (called V(s)) when using that strategy. Think of it as keeping track of \"how good\" each state is by observing what happens when you visit it many times.\n",
        "\n",
        "Instead of just tracking how good each state is, you now track how good each action is in each state. This is called Q(s,a), the action-value function.\n",
        "At the start, you try actions randomly (this is called exploration).\n",
        "Over time, you see which actions lead to the best outcomes and start choosing those actions more often (this is called exploitation).\n",
        "To keep improving, you mix random actions (exploration) and smart actions (exploitation) using a method called epsilon-greedy: most of the time, you pick the best-known action, but sometimes you try something random to learn more.\n",
        "After many games, the strategy becomes optimized because it picks the best actions based on what you’ve learned.\n",
        "Think of this as learning the best way to play the game by trial and error over thousands of games."
      ],
      "metadata": {
        "id": "d78Ey6jg0jZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    import gym\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "    mc_agent = MonteCarlo(env)\n",
        "    print(\"Initial Random Policy:\")\n",
        "    print(mc_agent.policy)\n",
        "\n",
        "    print(\"\\nPerforming MC Prediction...\")\n",
        "    mc_agent.mc_prediction(episodes=1000)\n",
        "    print(\"Estimated State Values:\")\n",
        "    print(dict(mc_agent.state_values))\n",
        "\n",
        "    print(\"\\nPerforming MC Control...\")\n",
        "    mc_agent.mc_control(episodes=1000)\n",
        "    print(\"Optimized Policy:\")\n",
        "    print(dict(mc_agent.policy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksJhZwsl0csN",
        "outputId": "4d818af6-456f-4bd2-cdd7-6eb034e6daa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Random Policy:\n",
            "defaultdict(<function MonteCarlo.__init__.<locals>.<lambda> at 0x7bc6f7981a20>, {})\n",
            "\n",
            "Performing MC Prediction...\n",
            "Estimated State Values:\n",
            "{306: -1.0, 427: -10.0, 224: -1.5727272727272728, 244: -1.3272727272727272, 124: -9.999999992357, 474: -10.0, 131: -2.442142857142857, 111: -2.461, 11: -1.8884285714285713, 31: -2.2574285714285716, 413: -10.0, 483: -10.0, 289: -10.0, 282: -10.0, 182: -90.99999992944923, 208: -10.0, 108: -90.99999992944923, 151: -10.0, 401: -1.0, 448: -1.0, 454: -1.4500000000000002, 434: -1.45, 351: -1.6299999999999997, 251: -1.27, 334: -10.0, 148: -10.0, 61: -10.0, 42: -1.0, 344: -10.0, 281: -1.1500000000000001, 381: -1.7499999999999998, 181: -9.999999992944923, 132: -1.0, 189: -90.99999992944923, 112: -9.999999992631365, 389: -1.09, 369: -1.8099999999999998, 171: -10.0, 191: -90.99999992944923, 363: -1.5142857142857142, 263: -1.3857142857142857, 491: -1.3857142857142857, 391: -1.5142857142857142, 447: -1.0, 284: -10.0, 264: -90.99999992944923, 409: -1.0, 309: -9.999999992683625, 94: -1.0, 386: -10.0, 486: -90.99999992944923, 268: -10.0, 53: -1.0, 153: -9.999999992709755, 52: -1.0, 152: -9.999999992944923, 213: -1.0, 113: -9.999999992720953, 492: -10.0, 41: -90.99999992136141, 141: -82.89999992631365, 252: -10.0, 272: -90.99999991831291, 172: -82.89999992374817, 72: -75.60999992944923, 206: -10.0, 12: -9.999999992944923, 186: -10.0, 286: -90.99999992944923, 273: -10.0, 471: -1.0, 371: -9.999999992944923, 82: -10.0, 304: -1.0, 494: -10.0, 374: -10.0, 54: -1.0, 34: -10.0, 14: -90.99999992944923, 303: -10.0, 453: -10.0, 93: -10.0, 353: -90.99999992944923, 126: -1.1, 106: -1.7999999999999998, 26: -9.999999992944923, 86: -1.45, 66: -1.45, 166: -90.99999992944923, 103: -10.0, 269: -10.0, 293: -90.99999992944923, 24: -9.999999992944923, 271: -10.0, 43: -10.0, 143: -90.99999991399329, 163: -82.89999992073926, 183: -75.60999992474584, 83: -69.04899992944922, 6: -10.0, 428: -10.0, 384: -90.99999992131991, 484: -82.89999992552976, 4: -10.0, 332: -1.0, 232: -9.99999999274895, 221: -1.3, 121: -1.5999999999999999, 294: -1.0, 167: -10.0, 267: -90.99999992944923, 92: -1.0, 154: -9.999999992944923, 188: -1.0, 168: -9.999999992552976, 446: -10.0, 233: -10.0, 348: -10.0, 248: -90.99999992944923, 493: -10.0, 452: -10.0, 81: -1.0, 482: -10.0, 341: -1.0, 96: -1.0, 84: -9.999999992552974, 333: -10.0, 433: -90.99999992944923, 249: -10.0, 229: -90.9999999209328, 329: -82.89999992608968, 3: -1.0, 46: -10.0, 146: -90.99999992944923, 64: -9.999999992944923, 403: -1.0, 22: -1.0, 382: -10.0, 17: -1.0, 1: -9.999999992474585, 88: -1.0, 451: -10.0, 431: -90.99999992944923, 407: -10.0, 327: -10.0, 227: -90.99999992474585, 247: -82.89999992944924, 421: -10.0, 23: -1.0, 71: -90.99999992944923, 352: -9.999999992944923, 347: -9.999999992944923, 472: -10.0, 441: -1.0, 48: -90.99999992944923, 463: -10.0, 262: -10.0, 242: -90.99999992944923, 462: -10.0, 253: -90.99999992944923, 63: -10.0, 9: -10.0, 109: -90.99999992944923, 373: -90.99999992552974, 367: -10.0, 387: -90.99999992944923, 89: -1.0, 326: -10.0, 49: -10.0, 69: -90.99999992944923, 313: -1.0, 292: -10.0, 127: -10.0, 107: -90.99999992731132, 47: -10.0, 2: -10.0, 442: -1.0, 422: -9.999999992552974, 322: -9.999999992944923, 231: -9.999999992944923, 432: -9.999999992944923, 328: -10.0, 274: -9.999999992317806, 254: -9.999999992944923, 302: -10.0, 402: -90.99999992944923, 394: -10.0, 149: -10.0, 308: -1.0, 8: -1.0, 33: -10.0, 283: -10.0, 204: -9.999999992944923, 74: -9.999999992422326, 174: -9.999999992944923, 392: -90.99999992944923, 426: -1.0, 314: -1.0, 468: -10.0, 488: -90.99999992944923, 68: -9.999999992944923, 362: -90.99999992944923, 27: -90.99999992944923, 226: -10.0, 161: -90.99999992608969, 261: -82.89999992944924, 142: -9.999999992944923, 301: -1.0, 184: -1.0, 164: -9.999999992552976, 144: -9.999999992944923, 349: -10.0, 449: -90.99999992944923, 343: -1.3857142857142857, 243: -1.514285714285714, 223: -9.999999992683623, 123: -9.999999992944923, 366: -10.0, 266: -90.99999992944923, 13: -1.0, 192: -75.60999992944923, 383: -1.0, 201: -1.0, 7: -1.0, 32: -9.999999992944923, 147: -90.99999992944923, 323: -9.999999992944923, 424: -10.0, 419: -1.0, 411: -1.9000000000000001, 102: -90.999999925312, 443: -10.0, 307: -10.0, 414: -1.0, 489: -9.999999992944923, 214: -9.999999992944923, 193: -10.0, 388: -10.0, 312: -1.0, 361: -1.0, 187: -1.0, 87: -9.999999992944923, 481: -10.0, 412: -10.0, 404: -1.0, 287: -10.0, 101: -9.999999992944923, 134: -90.99999992944923, 62: -9.999999992552974, 162: -9.999999992944923, 346: -90.99999992944923, 467: -1.45, 487: -1.45, 473: -1.0, 311: -1.0, 211: -1.9, 21: -1.0, 51: -1.0, 202: -10.0, 28: -9.999999992552976, 128: -9.999999992944923, 234: -90.99999992944923, 324: -1.0, 288: -10.0, 354: -10.0, 429: -75.60999992944923, 129: -82.89999992720952, 469: -1.0, 169: -90.99999992944923, 464: -75.60999992944923, 133: -9.999999992944923, 29: -75.60999992944923, 91: -10.0, 194: -1.0, 228: -90.99999992944923, 321: -10.0, 104: -10.0, 406: -10.0, 173: -9.999999992944923, 423: -10.0, 408: -10.0, 342: -10.0, 203: -1.0, 207: -82.89999992944924, 291: -1.0, 73: -9.999999992944923, 114: -10.0, 122: -82.89999992161026, 222: -75.60999992944923, 209: -9.999999992944923, 461: -10.0, 393: -82.89999992944924, 368: -10.0, 241: -75.60999992944923, 364: -10.0, 212: -9.999999992944923, 444: -90.99999992944923, 246: -10.0}\n",
            "\n",
            "Performing MC Control...\n",
            "Optimized Policy:\n",
            "{306: 2, 427: 3, 124: 2, 224: 4, 244: 3, 474: 2, 31: 5, 11: 4, 111: 3, 131: 2, 413: 0, 483: 3, 289: 2, 182: 2, 282: 1, 108: 3, 208: 3, 151: 4, 401: 0, 448: 3, 434: 0, 454: 0, 251: 4, 351: 5, 334: 0, 148: 2, 61: 5, 42: 1, 344: 2, 181: 5, 281: 2, 381: 5, 132: 0, 189: 0, 112: 3, 369: 3, 389: 2, 191: 4, 171: 1, 263: 5, 363: 3, 391: 2, 491: 3, 447: 5, 264: 4, 284: 2, 309: 5, 409: 0, 94: 5, 486: 4, 386: 2, 268: 3, 153: 3, 53: 3, 152: 5, 52: 4, 113: 5, 213: 3, 492: 0, 141: 3, 41: 3, 72: 0, 172: 3, 272: 0, 252: 2, 206: 2, 12: 5, 286: 3, 186: 2, 273: 4, 371: 4, 471: 0, 82: 2, 304: 4, 494: 1, 374: 2, 54: 1, 14: 1, 34: 2, 303: 1, 453: 4, 93: 1, 353: 5, 26: 1, 126: 2, 106: 3, 66: 1, 86: 0, 166: 4, 103: 3, 269: 5, 293: 2, 24: 2, 271: 4, 83: 1, 183: 2, 163: 5, 143: 3, 43: 5, 6: 3, 428: 2, 484: 4, 384: 2, 4: 0, 232: 2, 332: 3, 121: 2, 221: 1, 294: 1, 267: 4, 167: 5, 92: 1, 154: 3, 168: 1, 188: 5, 446: 0, 233: 3, 248: 4, 348: 2, 493: 0, 452: 3, 81: 1, 482: 4, 341: 2, 84: 1, 96: 0, 433: 5, 333: 4, 329: 2, 229: 3, 249: 1, 3: 1, 146: 4, 46: 2, 64: 3, 403: 4, 22: 1, 382: 5, 1: 0, 17: 0, 88: 3, 431: 3, 451: 3, 407: 3, 247: 1, 227: 1, 327: 3, 421: 4, 23: 3, 71: 0, 352: 2, 347: 2, 472: 3, 441: 4, 48: 1, 463: 2, 242: 0, 262: 3, 462: 0, 253: 1, 63: 1, 109: 5, 9: 1, 373: 5, 387: 4, 367: 5, 89: 0, 326: 3, 69: 2, 49: 1, 313: 2, 292: 4, 107: 3, 127: 4, 47: 4, 2: 2, 322: 3, 422: 3, 442: 3, 231: 3, 432: 0, 328: 3, 254: 3, 274: 1, 402: 0, 302: 2, 394: 3, 149: 3, 308: 2, 8: 5, 33: 1, 283: 2, 204: 4, 174: 3, 74: 1, 392: 2, 426: 5, 314: 4, 488: 5, 468: 0, 68: 1, 362: 3, 27: 1, 226: 3, 261: 0, 161: 0, 142: 5, 301: 2, 144: 4, 164: 5, 184: 3, 449: 3, 349: 0, 123: 3, 223: 1, 243: 0, 343: 1, 266: 1, 366: 4, 13: 0, 192: 2, 383: 3, 201: 5, 7: 5, 32: 4, 147: 5, 323: 3, 424: 3, 411: 1, 419: 0, 102: 3, 443: 3, 307: 3, 414: 4, 489: 5, 214: 5, 193: 2, 388: 5, 312: 5, 361: 4, 87: 3, 187: 5, 481: 3, 412: 5, 404: 0, 287: 2, 101: 3, 134: 5, 162: 1, 62: 1, 346: 4, 487: 1, 467: 4, 473: 5, 211: 3, 311: 2, 21: 5, 51: 5, 202: 3, 128: 2, 28: 1, 234: 1, 324: 5, 288: 2, 354: 5, 429: 0, 129: 4, 469: 5, 169: 1, 464: 5, 133: 2, 29: 1, 91: 1, 194: 4, 228: 4, 321: 3, 104: 2, 406: 5, 173: 4, 423: 0, 408: 2, 342: 2, 203: 2, 207: 3, 291: 3, 73: 1, 114: 3, 222: 1, 122: 2, 209: 3, 461: 4, 393: 5, 368: 4, 241: 1, 364: 3, 212: 3, 444: 4, 246: 0, 67: 1, 331: 2, 372: 3, 44: 4, 466: 0, 98: 0, 19: 1, 119: 0, 477: 0, 377: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Details (Taxi-v3):**\n",
        "State Space: 500 discrete states, representing the taxi’s position, the passenger’s location, and the destination.\n",
        "Action Space: 6 discrete actions:\n",
        "Move South (0), North (1), East (2), West (3), Pickup (4), Dropoff (5).\n",
        "Goal: Minimize the number of moves to pick up and drop off the passenger correctly."
      ],
      "metadata": {
        "id": "GBtzyqgy0Qj7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcVUEPtRxYFr"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}