{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorldEnv:\n",
        "    def __init__(self, grid_size=(5, 5), goal_state=(4, 4), start_state=(0, 0)):\n",
        "        self.grid_size = grid_size\n",
        "        self.goal_state = goal_state\n",
        "        self.start_state = start_state\n",
        "        self.current_state = start_state\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.num_actions = len(self.actions)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment to the start state.\"\"\"\n",
        "        self.current_state = self.start_state\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Takes an action and returns the next state, reward, and whether the goal is reached.\"\"\"\n",
        "        x, y = self.current_state\n",
        "        if action == 0:  # UP\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 1:  # DOWN\n",
        "            x = min(self.grid_size[0] - 1, x + 1)\n",
        "        elif action == 2:  # LEFT\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 3:  # RIGHT\n",
        "            y = min(self.grid_size[1] - 1, y + 1)\n",
        "\n",
        "        next_state = (x, y)\n",
        "        reward = 1 if next_state == self.goal_state else -0.1\n",
        "        done = next_state == self.goal_state\n",
        "        self.current_state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def sample_action(self):\n",
        "        \"\"\"Samples a random action.\"\"\"\n",
        "        return np.random.choice(self.num_actions)\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Prints the grid world with the agent's current position.\"\"\"\n",
        "        grid = [['.' for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
        "        x, y = self.current_state\n",
        "        grid[x][y] = 'A'\n",
        "        gx, gy = self.goal_state\n",
        "        grid[gx][gy] = 'G'\n",
        "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
        "        print()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorldEnv()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"Initial Environment:\")\n",
        "    env.render()\n",
        "\n",
        "    while not done:\n",
        "        action = env.sample_action()\n",
        "        next_state, reward, done = env.step(action)\n",
        "        print(f\"Action Taken: {env.actions[action]} | Reward: {reward} | Next State: {next_state}\")\n",
        "        env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryzEmwds3JAi",
        "outputId": "b7712521-85a3-44d5-ecb4-419548c868e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Environment:\n",
            "A . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 1)\n",
            ". A . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 2)\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 3)\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: UP | Reward: -0.1 | Next State: (0, 3)\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 4)\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: UP | Reward: -0.1 | Next State: (0, 4)\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 4)\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: UP | Reward: -0.1 | Next State: (0, 4)\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (0, 4)\n",
            ". . . . A\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: LEFT | Reward: -0.1 | Next State: (0, 3)\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: DOWN | Reward: -0.1 | Next State: (1, 3)\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: LEFT | Reward: -0.1 | Next State: (1, 2)\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: DOWN | Reward: -0.1 | Next State: (2, 2)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (2, 3)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: DOWN | Reward: -0.1 | Next State: (3, 3)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (3, 4)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . A\n",
            ". . . . G\n",
            "\n",
            "Action Taken: LEFT | Reward: -0.1 | Next State: (3, 3)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: LEFT | Reward: -0.1 | Next State: (3, 2)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: UP | Reward: -0.1 | Next State: (2, 2)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: DOWN | Reward: -0.1 | Next State: (3, 2)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . A . .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (3, 3)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . A .\n",
            ". . . . G\n",
            "\n",
            "Action Taken: RIGHT | Reward: -0.1 | Next State: (3, 4)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . A\n",
            ". . . . G\n",
            "\n",
            "Action Taken: DOWN | Reward: 1 | Next State: (4, 4)\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . .\n",
            ". . . . G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViMJJdjQ3JYq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}